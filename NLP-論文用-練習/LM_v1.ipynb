{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # 數據處理庫\n",
    "import nltk  # Natural Language Toolkit (NLTK) NLP庫，有很多 NLP 工具\n",
    "from nltk.tokenize import word_tokenize # word_tokenize: 此函數可把一段文本切割為單詞 (tokenization)\n",
    "from nltk.corpus import wordnet  # 語料庫(corpora), wordnet: 語義詞典，用於提供詞的詞性、同義詞、反義詞等\n",
    "from nltk.stem import WordNetLemmatizer # WordNetLemmatizer: 用於詞形還原\n",
    "# 詞形還原：英文的名詞有複數型態，動詞則有過去式或進行式等，為了使單字標準化。使用 WordNetLemmatizer()將字詞變回原形。\n",
    "#  1. 動詞 \"running\" 的詞根是 \"run\"。\n",
    "#  2. 名詞 \"better\" 的詞根是 \"good\"。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下載 NLTK 資源\n",
    "\n",
    "NLTK的許多功能依賴於大型語言資源，例如語料庫(corpora)、詞典和模型，這些資源因為體積較大或因為頻繁更新，通常不會內建於 NLTK 庫中。因此，要使用時需要單獨下載資源。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger') # nltk.pos_tag() 背後運行的標註器，是一個機器學習模型，這個模型已經通過大量語料庫訓練過，能夠準確地給文本進行詞性標註。它不是一個數據集，而是一個預訓練模型\n",
    "nltk.download('punkt') # 無監督學習模型，將文本按句子或單詞分割，不依賴詞典。是一個預訓練模型。\n",
    "nltk.download('wordnet') # 一個詞典數據集，它不僅提供單詞和詞義的對應，還能提供同義詞、反義詞等信息。用於詞形還原和語義分析(lemmatizer)\n",
    "\n",
    "# 初始化詞形還原器\n",
    "lemmatizer = WordNetLemmatizer() # instantiation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詞性標籤(POS tags) \n",
    "\n",
    "詞性標籤是對句子中的每個單詞進行\"語法分類\"的過程。標籤告訴我們每個單詞是名詞、動詞、形容詞還是副詞等。\n",
    "    \n",
    "1. NLTK 的詞性標籤： 使用 NN(名詞)、VB(動詞)、JJ(形容詞)等。\n",
    "2. WordNet 的詞性標籤： 如 wordnet.NOUN(名詞)、wordnet.VERB(動詞)、wordnet.ADJ(形容詞)等。\n",
    "    \n",
    "為了讓 WordNetLemmatizer 知道每個單詞的詞性，需要一個從 NLTK tag 轉換到 WordNet tag。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "# 將 NLTK 的詞性標籤轉換為 WordNet 的詞性標籤\n",
    "def get_wordnet_pos(tag):  # def a function with parameter: tag\n",
    "    if tag.startswith('J'): # startswith(): 檢查是否以指定字母開頭，此處檢查 argument的開頭是否為 \"J\"\n",
    "        return wordnet.ADJ # True: return WordNet's adj tag (wordnet.ADJ) \n",
    "    elif tag.startswith('V'): # 檢查 argument的開頭是否為 \"V\"\n",
    "        return wordnet.VERB # True: return WordNet's verb tag (wordnet.VERB)\n",
    "    elif tag.startswith('N'): # 檢查 argument的開頭是否為 \"N\"\n",
    "        return wordnet.NOUN # True: return WordNet's noun tag (wordnet.NOUN)\n",
    "    elif tag.startswith('R'): # 檢查 argument的開頭是否為 \"R\"\n",
    "        return wordnet.ADV # True: return WordNet's adv tag (wordnet.ADV)\n",
    "    else:\n",
    "        return wordnet.NOUN  # 若非 \"J\", \"V\", \"N\", \"R\"，則 return wordnet.NOUN (default = wordnet.NOUN) \n",
    "\n",
    "    \n",
    "# 改進的詞形還原函數\n",
    "def lemmatize_words(words): # def a function with parameter: words\n",
    "    lemmatized = set() # asign a empty set to variable \"lemmatized\"\n",
    "    for word in words: # iterate over \"words\"\n",
    "        word = word.lower()  # lowercase \n",
    "        tagged = nltk.pos_tag([word]) # nltk.pos_tag: 對 word 進行詞性標註，會返回一個 list [(\"單詞\", \"詞性\")]\n",
    "                                      # nltk.pos_tag() return 的值為用 list 包起來的 tuple，例如 [('run', 'VB'),('jump', 'VB')]\n",
    "        wordnet_pos = get_wordnet_pos(tagged[0][1])  # 用剛剛 def function 把 return list 中第一個 tuple 中的 第二個 element 抓出來，也就是 tag 的部分，return wordnet 版本 tag\n",
    "        lemmatized.add(lemmatizer.lemmatize(word, pos=wordnet_pos))  # 進行詞形還原\n",
    "        # lemmatizer.lemmatize(): 對 word 進行詞形還原。會根據提供的詞性(wordnet_pos)將單詞還原成基本形式。\n",
    "        # Ex. running -> wordnet_pos=wordnet_VERB -> run\n",
    "        # 被還原的詞被加入 lemmatize set \n",
    "    return lemmatized # retrun 全部被還原完畢的 set\n",
    "\n",
    "\n",
    "def load_sentiment_words(file_path): \n",
    "    df = pd.read_excel(file_path) # pandas read_excel 讀取 file_path excel，函數會返回一個 DataFrame 對象 (instantiation) \n",
    "    positive_words = set(df.iloc[:, 1].dropna().astype(str))  # df.iloc[row_index, column_index]，這邊抓第一個 column 全部資料\n",
    "    negative_words = set(df.iloc[:, 0].dropna().astype(str))  # dropna(): 去除 null, astype(str): 轉成 string，最後把結果轉成 set(可以去除重複值)\n",
    "    return positive_words, negative_words # return 這兩個清乾淨的 set\n",
    "\n",
    "\n",
    "def determine_sentiment(sentence, positive_words, negative_words):\n",
    "    words = word_tokenize(sentence.lower()) # sentence lowercase，並使用 word_tokenzie()切割成單詞\n",
    "    has_positive = any(word in positive_words for word in words) # generator expression: return True or False, 檢查 words 中的每個 word 是否有在 positive_words 中 \n",
    "    has_negative = any(word in negative_words for word in words) # any(): 只要其中有一個 True return True；不再檢查剩餘的單詞。\n",
    "\n",
    "    # if there is a negative word, its negative(?)\n",
    "    if has_negative: # has_negative return True\n",
    "        return 'Negative' \n",
    "    elif has_positive and not has_negative: # has_positive return True and has_negative return False\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "    \n",
    "\n",
    "# 載入詞庫，經過剛剛 def 的 function: load_sentiment_words()，會 return 2 個清洗乾淨的 set\n",
    "positive_words, negative_words = load_sentiment_words(r'C:\\Users\\user\\Desktop\\Python\\Thesis\\NLP\\LM字典情緒詞庫.xlsx')  \n",
    "\n",
    "# 還原詞庫，經過 def func: lemmatize_words(): return 完整還原完畢的 set\n",
    "positive_words = lemmatize_words(positive_words)\n",
    "negative_words = lemmatize_words(negative_words)\n",
    "\n",
    "# 測試句子，經過 def func: determine_sentiment()，return 'Negative' or 'Positive' or 'Neutral'\n",
    "test_sentence = \"Previously Unknown Mozart Song Discovered in German Library After 200 Years.\"\n",
    "sentiment = determine_sentiment(test_sentence, positive_words, negative_words)\n",
    "print(f\"{sentiment}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update - 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test one:\n",
      "positive_words: {'enjoy', 'advantageously', 'conclusive', 'collaborate', 'efficiently', 'satisfaction', 'breakthrough', 'invention', 'enthusiasm', 'able', 'inspirational', 'impressive', 'influential', 'profitability', 'best', 'attractiveness', 'proficiently', 'successfully', 'insightful', 'rebound', 'satisfactorily', 'honorable', 'friendly', 'desire', 'boost', 'versatility', 'beneficially', 'efficient', 'preeminence', 'successful', 'stabilize', 'distinctive', 'exclusiveness', 'enthusiastically', 'attains', 'lucrative', 'creativeness', 'beautifully', 'happy', 'premier', 'collaborator', 'excitement', 'empowers', 'leadership', 'transparency', 'receptive', 'spectacularly', 'valuable', 'adequately', 'enjoyably', 'unsurpassed', 'spectacular', 'innovation', 'outperforms', 'surpasses', 'smooth', 'advancement', 'conclusively', 'greatly', 'tremendously', 'loyal', 'improve', 'advantageous', 'progress', 'happiest', 'assures', 'pleasure', 'happily', 'vibrant', 'ideal', 'prosperity', 'easy', 'popularity', 'well', 'improvement', 'accomplishes', 'achieves', 'winner', 'creatively', 'fantastic', 'profitable', 'incredibly', 'enjoys', 'inventive', 'stability', 'pleasant', 'prospers', 'favorably', 'profitably', 'impress', 'improves', 'achievement', 'enjoyable', 'courteous', 'satisfies', 'success', 'collaboration', 'charitable', 'abundance', 'premiere', 'impressively', 'enable', 'constructive', 'benefiting', 'inventor', 'revolutionize', 'ingenuity', 'distinctiveness', 'optimistic', 'innovate', 'lead', 'plentiful', 'encourage', 'distinctively', 'beneficial', 'honor', 'abundant', 'alliance', 'dependability', 'favor', 'excellence', 'diligent', 'positive', 'win', 'assure', 'prestige', 'good', 'positively', 'excel', 'destine', 'enjoyment', 'exemplary', 'complimentary', 'bolster', 'empower', 'opportunity', 'collaborates', 'resolve', 'favorable', 'delight', 'enhancement', 'diligently', 'prosper', 'advance', 'solve', 'inventiveness', 'accomplishment', 'stable', 'advantage', 'conducive', 'achieve', 'boom', 'creativity', 'high', 'tremendous', 'greatness', 'worthy', 'favorite', 'strong', 'innovator', 'effective', 'delighted', 'strengthens', 'desirable', 'innovativeness', 'brilliant', 'stabilizes', 'revolutionizes', 'easily', 'proactive', 'prestigious', 'exceptionally', 'exceptional', 'efficiency', 'preeminent', 'innovates', 'excellent', 'proficiency', 'perfect', 'vibrancy', 'incredible', 'inspiration', 'attain', 'smoothly', 'delightful', 'excite', 'attainment', 'satisfactory', 'innovative', 'creative', 'meritorious', 'satisfy', 'integrity', 'collaborative', 'gain', 'constructively', 'invent', 'dream', 'enables', 'unparalleled', 'proactively', 'accomplish', 'happiness', 'proficient', 'exclusively', 'perfectly', 'beautiful', 'prosperous', 'informative', 'acclaim', 'despite', 'delightfully', 'distinction', 'popular', 'excels', 'unmatched', 'exclusivity', 'pleased', 'pleasantly', 'outperform', 'compliment', 'reward', 'confident', 'solves', 'enthusiastic', 'exclusive', 'strengthen', 'impressed', 'regain', 'great', 'benefitting', 'encouragement', 'versatile', 'benefit', 'surpass', 'succeed', 'strength', 'attractive', 'encourages', 'upturn', 'enhance', 'dependable', 'stabilization', 'superior', 'enhances', 'satisfied'}\n",
      "tag before transfered: {'IN', 'VB', 'NN', 'JJS', 'JJ', 'NNS', 'RB'}\n",
      "tag after tranfered{'n', 'r', 'a', 'v'}\n",
      "words after lemmatized: {'enjoy', 'advantageously', 'conclusive', 'efficiently', 'collaborate', 'satisfaction', 'breakthrough', 'invention', 'enthusiasm', 'able', 'inspirational', 'impressive', 'influential', 'profitability', 'best', 'attractiveness', 'proficiently', 'successfully', 'insightful', 'rebound', 'satisfactorily', 'honorable', 'friendly', 'desire', 'boost', 'versatility', 'beneficially', 'efficient', 'preeminence', 'successful', 'stabilize', 'distinctive', 'exclusiveness', 'enthusiastically', 'lucrative', 'creativeness', 'beautifully', 'happy', 'premier', 'collaborator', 'excitement', 'empowers', 'leadership', 'transparency', 'receptive', 'spectacularly', 'valuable', 'adequately', 'enjoyably', 'unsurpassed', 'spectacular', 'innovation', 'outperforms', 'surpasses', 'smooth', 'advancement', 'conclusively', 'greatly', 'tremendously', 'loyal', 'improve', 'advantageous', 'progress', 'happiest', 'assures', 'pleasure', 'happily', 'vibrant', 'ideal', 'easy', 'popularity', 'prosperity', 'well', 'improvement', 'accomplishes', 'achieves', 'winner', 'creatively', 'fantastic', 'profitable', 'incredibly', 'enjoys', 'inventive', 'stability', 'pleasant', 'prospers', 'favorably', 'profitably', 'impress', 'improves', 'achievement', 'enjoyable', 'courteous', 'satisfies', 'success', 'collaboration', 'charitable', 'abundance', 'premiere', 'impressively', 'enable', 'constructive', 'benefiting', 'inventor', 'revolutionize', 'ingenuity', 'distinctiveness', 'optimistic', 'innovate', 'lead', 'plentiful', 'encourage', 'distinctively', 'beneficial', 'honor', 'abundant', 'alliance', 'dependability', 'favor', 'excellence', 'diligent', 'positive', 'win', 'assure', 'prestige', 'good', 'positively', 'excel', 'destine', 'enjoyment', 'exemplary', 'complimentary', 'bolster', 'empower', 'opportunity', 'collaborates', 'resolve', 'favorable', 'delight', 'enhancement', 'diligently', 'prosper', 'advance', 'solve', 'inventiveness', 'accomplishment', 'stable', 'advantage', 'conducive', 'achieve', 'boom', 'creativity', 'high', 'tremendous', 'greatness', 'worthy', 'favorite', 'strong', 'innovator', 'effective', 'delighted', 'strengthens', 'desirable', 'innovativeness', 'brilliant', 'stabilizes', 'revolutionizes', 'easily', 'proactive', 'prestigious', 'exceptionally', 'exceptional', 'efficiency', 'preeminent', 'innovates', 'excellent', 'proficiency', 'perfect', 'vibrancy', 'incredible', 'inspiration', 'attain', 'smoothly', 'delightful', 'excite', 'attainment', 'satisfactory', 'innovative', 'creative', 'meritorious', 'satisfy', 'integrity', 'collaborative', 'superior', 'gain', 'constructively', 'invent', 'dream', 'enables', 'unparalleled', 'proactively', 'accomplish', 'happiness', 'proficient', 'exclusively', 'perfectly', 'beautiful', 'prosperous', 'informative', 'acclaim', 'despite', 'delightfully', 'distinction', 'popular', 'excels', 'unmatched', 'exclusivity', 'pleased', 'pleasantly', 'outperform', 'compliment', 'reward', 'confident', 'solves', 'enthusiastic', 'exclusive', 'strengthen', 'impressed', 'regain', 'great', 'benefitting', 'encouragement', 'versatile', 'benefit', 'surpass', 'succeed', 'strength', 'attractive', 'encourages', 'upturn', 'enhance', 'dependable', 'stabilization', 'attains', 'enhances', 'satisfied'}\n",
      "positive_words - words after lemmatized: set()\n",
      "\n",
      "Test two:\n",
      "['“', 'For', 'the', 'record', ',', 'I', '’', 'm', 'not', 'Satoshi', ',', '”', 'Peter', 'Todd', ',', 'a', 'bitcoin', 'core', 'developer', ',', 'told', 'CNN', 'in', 'a', 'statement', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test one \"Why use lemmatized function to positive and negative words?\"\n",
    "def test_lemmatize(words):\n",
    "    lemmatized_test = set()\n",
    "    tag_before = set()\n",
    "    tag_after = set()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        tag = nltk.pos_tag([word])\n",
    "        tag_before.add(tag[0][1])\n",
    "        wordnet_pos_test = get_wordnet_pos(tag[0][1])\n",
    "        tag_after.add(wordnet_pos_test)\n",
    "        lemmatized_test.add(lemmatizer.lemmatize(word, pos=wordnet_pos_test))\n",
    "    return [tag_before, tag_after, lemmatized_test]\n",
    "\n",
    "\n",
    "result = test_lemmatize(positive_words)\n",
    "print(f\"Test one:\\npositive_words: {positive_words}\\ntag before transfered: {result[0]}\\ntag after tranfered{result[1]}\\nwords after lemmatized: {result[2]}\")\n",
    "print(f\"positive_words - words after lemmatized: {positive_words.symmetric_difference(result[2])}\")\n",
    "\n",
    "# test two \"How does nltk's word_tokenize function works?\"\n",
    "sentence_test = \"“For the record, I’m not Satoshi,” Peter Todd, a bitcoin core developer, told CNN in a statement.\"\n",
    "word = word_tokenize(sentence_test)\n",
    "print(f\"\\nTest two:\\n{word}\")\n",
    "\n",
    "# test three \"The mechanism of LM code?\"\n",
    "sentence_txt = \"good, good, bad\"\n",
    "positive = {'good'}\n",
    "negative = {'bad'}\n",
    "determine_sentiment(sentence_txt, positive, negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question one: \n",
    "lemmatized function should be applied to input sentence. \n",
    "## Question two: \n",
    "function `word_tokenize` can't deal with stop word.\n",
    "## Question three: \n",
    "The determine_sentiment function's decision-making mechanism should use the word counts of each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2355 entries, 0 to 2354\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   negative word  2355 non-null   object\n",
      " 1   positive word  354 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 36.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative word</th>\n",
       "      <th>positive word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>ABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABANDONED</td>\n",
       "      <td>ABUNDANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABANDONING</td>\n",
       "      <td>ABUNDANT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "      <td>ACCLAIMED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABANDONMENTS</td>\n",
       "      <td>ACCOMPLISH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABANDONS</td>\n",
       "      <td>ACCOMPLISHED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ABDICATED</td>\n",
       "      <td>ACCOMPLISHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABDICATES</td>\n",
       "      <td>ACCOMPLISHING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ABDICATING</td>\n",
       "      <td>ACCOMPLISHMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ABDICATION</td>\n",
       "      <td>ACCOMPLISHMENTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  negative word    positive word\n",
       "0       ABANDON             ABLE\n",
       "1     ABANDONED        ABUNDANCE\n",
       "2    ABANDONING         ABUNDANT\n",
       "3   ABANDONMENT        ACCLAIMED\n",
       "4  ABANDONMENTS       ACCOMPLISH\n",
       "5      ABANDONS     ACCOMPLISHED\n",
       "6     ABDICATED     ACCOMPLISHES\n",
       "7     ABDICATES    ACCOMPLISHING\n",
       "8    ABDICATING   ACCOMPLISHMENT\n",
       "9    ABDICATION  ACCOMPLISHMENTS"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\user\\Desktop\\Python\\Thesis\\NLP\\LM字典情緒詞庫.xlsx\")\n",
    "df.info()\n",
    "df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取CSV，進行分析並保存結果\n",
    "def analyze_csv(input_file, output_file): \n",
    "    df = pd.read_csv(input_file) # pandas read_csv() 讀取 csv ，返回一個 DataFrame 對象 (instantiation)\n",
    "    df['LM label'] = df['sentence'].apply(lambda x: determine_sentiment(x, positive_words, negative_words)) # 把 sentence 欄位下的每一 row 去做 determine_sentiment(返回情緒)，並放入新欄位\"LM label\" \n",
    "    # lambda: anonymous function，(lambda parameter, return)\n",
    "    # .apply(): 將某個函數應用到 DataFrame 或 Series 的每一個元素上，x 為 sentence欄位的每一 row \n",
    "    df.to_csv(output_file, index=False) # 把 sentence 與對應的情緒保存到另一個 CSV 文件中(output_file連結, 不用 index 欄位)\n",
    "\n",
    "# 執行分析 \n",
    "analyze_csv(r'C:\\Users\\user\\Desktop\\Python\\Thesis\\NLP\\data_before_LM.csv', r\"C:\\Users\\user\\Desktop\\Python\\Thesis\\NLP\\data_after_LM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "#### 1. Accuracy: \n",
    "正確的預測數 / 總數據數\n",
    "#### 2. Precision： \n",
    "預測正確positive / 所有預測為positive -> 所有預測positive的樣本中，有多少是真的positive\n",
    "#### 3. Recall: \n",
    "預測正確陽 / (預測正確positive + 預測錯誤positive) -> 即所有positive樣本中，被召回了多少positive\n",
    "#### 4. F1 Score: \n",
    "Precision 和 Recall 的調和平均數\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 20\n",
      "Number of mismatches: 17\n",
      "Accuracy of the model predictions: 0.15\n",
      "Precision: 0.16\n",
      "Recall: 0.12\n",
      "F1 Score: 0.14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score # metric\n",
    "\n",
    "# 讀取 CSV 檔案\n",
    "file_path = r'C:\\Users\\user\\Desktop\\Python\\Thesis\\NLP\\metric.csv'\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1') # 以 ISO-8859-1 編碼方式讀取csv文件並返回 Dataframe (instantiation)\n",
    "\n",
    "# 計算不匹配的次數\n",
    "# 在 pandas，使用 Boolean comparison 對一整 column 進行操作時，這些運算會自動逐 row 應用到每個 element。\n",
    "mismatches = data['researcher2 label'] != data['LM label'] # researcher2 label 應為人工label的部分，若 != LM label則為 mismatch (True)\n",
    "mismatch_count = mismatches.sum() # 加總 mislable\n",
    "total_data = data['researcher2 label'] == data['researcher2 label'] # True\n",
    "data_count = total_data.sum() # total count of the data \n",
    "\n",
    "\n",
    "\n",
    "# 計算準確率\n",
    "accuracy = accuracy_score(data['researcher2 label'], data['LM label'])\n",
    "\n",
    "'''\n",
    "average='macro': 把每個類別的指標算出來後，再進行簡單的算術平均。\n",
    "Ex. \n",
    "    三個類別 A、B、C 精確率分別為 0.90、0.70、0.80\n",
    "    macro precision = (0.90 + 0.70 + 0.80) / 3 \n",
    "\n",
    "不論這三個類別的數據量是否相同，最終結果都是對三個類別的平均。\n",
    "'''\n",
    "# 計算精確率\n",
    "precision = precision_score(data['researcher2 label'], data['LM label'], average='macro')\n",
    "# 計算召回率\n",
    "recall = recall_score(data['researcher2 label'], data['LM label'], average='macro')\n",
    "# 計算 F1 分數\n",
    "f1 = f1_score(data['researcher2 label'], data['LM label'], average='macro')\n",
    "\n",
    "\n",
    "print(f\"Number of data: {data_count}\")\n",
    "print(f\"Number of mismatches: {mismatch_count}\")\n",
    "print(f\"Accuracy of the model predictions: {accuracy:.2f}\") # .2f: 小數點後2位\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 額外加的\n",
    "生成一個拿去跑的csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "with open(\"metric.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"researcher2 label\", \"LM label\"])\n",
    "    for row in range(20):\n",
    "        row = [random.randint(-1, 1), random.randint(-1, 1)]\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype\n",
      "---  ------             --------------  -----\n",
      " 0   researcher2 label  20 non-null     int64\n",
      " 1   LM label           20 non-null     int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 452.0 bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>researcher2 label</th>\n",
       "      <th>LM label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   researcher2 label  LM label\n",
       "0                  0         1\n",
       "1                 -1        -1\n",
       "2                  0         1\n",
       "3                  1        -1\n",
       "4                  1        -1\n",
       "5                 -1         1\n",
       "6                  1         1\n",
       "7                  1         0\n",
       "8                  1        -1\n",
       "9                 -1         1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\user\\Desktop\\Python\\Thesis\\NLP\\metric.csv\")\n",
    "df.info()\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
